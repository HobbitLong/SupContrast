{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create some sample data\n",
    "output = torch.randn(32, )  # Sample output tensor, 5 samples, 3 classes\n",
    "target = torch.tensor([1, 2, 0, 2, 1])  # Sample target tensor, ground truth labels\n",
    "\n",
    "# Call the accuracy function\n",
    "top1, top5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1.item():.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /Users/nina/Library/Python/3.9/lib/python/site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: filelock in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torch==2.0.1->torchvision) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torch==2.0.1->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nina/Library/Python/3.9/lib/python/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5560, -1.4274, -0.2668, -0.0637, -1.0900],\n",
      "        [ 1.4864, -0.0651,  0.1268,  0.4204, -0.2589],\n",
      "        [ 0.4242, -0.0728,  0.4617, -0.8344,  0.3453],\n",
      "        [-0.3346,  0.2447,  0.3663, -1.8127, -0.4779],\n",
      "        [-1.0697, -0.2619, -0.4329, -0.8537, -0.1789],\n",
      "        [ 1.3182, -1.8663,  0.2477, -0.1045,  0.5833],\n",
      "        [ 0.0652, -0.2209, -1.3188, -1.5880,  0.2209],\n",
      "        [ 1.7836,  0.8594,  1.0216, -0.1526,  0.1906]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.randn(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data loader\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "def set_loader(data_folder, batch_size): \n",
    "    input_shape = (3, 128, 128)\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "            transforms.Resize(int(input_shape[1] * 156 / 128)),\n",
    "            transforms.RandomCrop(input_shape[1:]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                            std=[0.5, 0.5, 0.5])\n",
    "        ])  \n",
    "\n",
    "    #assume two crop is not relevant \n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=data_folder,\n",
    "        transform=train_transform\n",
    "    )\n",
    "\n",
    "    train_sampler = None\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n",
    "        pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    print(\"train_loader length: \", len(train_loader))\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader length:  750\n"
     ]
    }
   ],
   "source": [
    "train_folder = \"./data/test/African/\"\n",
    "train_loader = set_loader(train_folder, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.randint(0, 500, (8,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[402,  55, 366, 240, 440, 440, 418,  55],\n",
      "        [416, 108,  75, 366, 192, 309, 402, 108],\n",
      "        [129, 314, 197, 190, 486, 154, 355,  23],\n",
      "        [428,  75, 414, 341,  44, 143, 428, 314],\n",
      "        [ 81, 240, 320, 176, 416, 416, 201, 492]]) torch.Size([5, 8])\n",
      "tensor([[262],\n",
      "        [423],\n",
      "        [177],\n",
      "        [429],\n",
      "        [472],\n",
      "        [214],\n",
      "        [155],\n",
      "        [240]]) torch.Size([8, 1])\n",
      "tensor([[262, 262, 262, 262, 262],\n",
      "        [423, 423, 423, 423, 423],\n",
      "        [177, 177, 177, 177, 177],\n",
      "        [429, 429, 429, 429, 429],\n",
      "        [472, 472, 472, 472, 472],\n",
      "        [214, 214, 214, 214, 214],\n",
      "        [155, 155, 155, 155, 155],\n",
      "        [240, 240, 240, 240, 240]]) torch.Size([8, 5])\n",
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "[tensor([0.]), tensor([0.])]\n"
     ]
    }
   ],
   "source": [
    "#randome int of batch size 8, from 500 classes\n",
    "\n",
    "topk = (1,5)\n",
    "batch_size = 8\n",
    "pred = torch.tensor([[402,  55, 366, 240, 440, 440, 418,  55],\n",
    "        [416, 108,  75, 366, 192, 309, 402, 108],\n",
    "        [129, 314, 197, 190, 486, 154, 355,  23],\n",
    "        [428,  75, 414, 341,  44, 143, 428, 314],\n",
    "        [ 81, 240, 320, 176, 416, 416, 201, 492]])\n",
    "print(pred, pred.shape)\n",
    "\n",
    "for idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "    pred = pred.t()\n",
    "    target = target.view(-1, 1)\n",
    "    print(target, target.shape)\n",
    "    target = target.expand_as(pred)\n",
    "    print(target, target.shape)\n",
    "\n",
    "    correct = pred.eq(target)\n",
    "    print(correct)\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        print(correct_k)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    \n",
    "    print(res)\n",
    "    break \n",
    "\n",
    "    # print(idx, data.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True])\n",
      "torch.Size([1])\n",
      "tensor([True])\n",
      "tensor([1.])\n",
      "tensor([True])\n",
      "tensor([ True, False, False, False, False])\n",
      "torch.Size([5])\n",
      "tensor([ True, False, False, False, False])\n",
      "tensor([1.])\n",
      "tensor([ True, False, False, False, False])\n",
      "[tensor([12.5000]), tensor([12.5000])]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "correct = torch.tensor([True, False, False, False, False, False, False, False],\n",
    "                       [False, False, False, False, False, False, False, False],\n",
    "                       [True, False, False, False, False, False, False, False])\n",
    "for k in topk:\n",
    "    print(correct[:k])\n",
    "    print(correct[:k].shape)\n",
    "    print(correct[:k].reshape(-1))\n",
    "    print(correct[:k].reshape(-1).float().sum(0, keepdim=True))\n",
    "    print(correct[:k].view(-1))\n",
    "    correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "    res.append(correct_k.mul_(100.0 / batch_size))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
