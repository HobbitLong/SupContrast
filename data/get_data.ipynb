{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os  \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(identity_selection, save_path, file_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    identity_selection.to_csv(save_path + file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def get_all_data_csv(data_path):\n",
    "    '''get csv of all data, race, id, image_name'''\n",
    "    coloumns = ['image_name', 'id', 'race']\n",
    "    print(data_path)\n",
    "\n",
    "    df = pd.DataFrame(columns=coloumns)\n",
    "    all_image = []\n",
    "    race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "    label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "    for race in race_to_label.keys():\n",
    "        race_folder = os.path.join(data_path+'/', race)\n",
    "        print(race_folder)\n",
    "        race_label = race_to_label[race]\n",
    "        for id_folder in os.listdir(race_folder):\n",
    "            if not 'm.' in id_folder:\n",
    "                continue\n",
    "            all_id_image = []\n",
    "\n",
    "            id_path = os.path.join(race_folder, id_folder)\n",
    "            id = id_folder.split(\"/\")[0]\n",
    "\n",
    "            for image in os.listdir(id_path):\n",
    "                all_id_image.append(image)\n",
    "            for image in all_id_image:\n",
    "                all_image.append([image, id, race_label])\n",
    "        \n",
    "    df = pd.DataFrame(all_image, columns=coloumns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_csv = get_all_data_csv(\"./race_per_7000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_per_race(race_df, image_per_id_limit=60, total_img=30000, id_per_race=500):\n",
    "    '''\n",
    "    get a new df with 60 image per id\n",
    "    draw id_per_race uniformly \n",
    "    draw image_per_id uniformly\n",
    "    '''\n",
    "    # Group by ID and count the number of images per ID\n",
    "    data_grouped = race_df.groupby(\"id\").count()\n",
    "    \n",
    "    # Filter IDs with at least 'image_per_id_limit' images\n",
    "    data_grouped = data_grouped[data_grouped[\"image_name\"] >= image_per_id_limit]\n",
    "    \n",
    "    # Get a list of IDs that meet the image count criteria\n",
    "    eligible_ids = data_grouped.index.tolist()\n",
    "    \n",
    "    # Shuffle the list of eligible IDs\n",
    "    random.shuffle(eligible_ids)\n",
    "    \n",
    "    # Take the first 'id_per_race' IDs to ensure uniform distribution\n",
    "    selected_ids = eligible_ids[:id_per_race]\n",
    "    \n",
    "    # Filter the DataFrame to include only rows with selected IDs\n",
    "    filtered_id_df = race_df[race_df[\"id\"].isin(selected_ids)]\n",
    "    \n",
    "    # Randomly sample images from each selected ID to meet the 'total_img' requirement\n",
    "    num_selected_images = 0\n",
    "    selected_rows = []\n",
    "    \n",
    "    for id in selected_ids:\n",
    "        id_df = filtered_id_df[filtered_id_df[\"id\"] == id]\n",
    "        num_images_for_id = min(image_per_id_limit, total_img - num_selected_images)\n",
    "        \n",
    "        # Randomly sample 'num_images_for_id' images for the current ID\n",
    "        sampled_rows = id_df.sample(n=num_images_for_id, random_state=42)\n",
    "        selected_rows.extend(sampled_rows.values)\n",
    "        num_selected_images += num_images_for_id\n",
    "        \n",
    "        if num_selected_images >= total_img:\n",
    "            break\n",
    "    \n",
    "    # Create a new DataFrame from the selected rows\n",
    "    new_df = pd.DataFrame(selected_rows, columns=['image_name', 'id', 'race'])\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced(data, image_per_id_limit=60, total_image=120000, id_per_race=500): \n",
    "    race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "    label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "    balanced_data = pd.DataFrame()\n",
    "\n",
    "    for race in race_to_label.keys():\n",
    "        race_label = race_to_label[race]\n",
    "        cur_race_data = data[data['race']==race_label]\n",
    "        race_balanced_df = get_balanced_per_race(cur_race_data, image_per_id_limit, total_image, id_per_race)\n",
    "        balanced_data = pd.concat([balanced_data, race_balanced_df])\n",
    "    \n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = get_balanced(data, 60, 120000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(balanced_data))\n",
    "print(balanced_data.head())\n",
    "print(balanced_data.groupby('id').count()['image_name'])\n",
    "print(balanced_data.groupby('race').count()['image_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(balanced_data, \"./\", \"balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out balanced data and get disjoint_balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = pd.read_csv(\"./balanced_data.csv\")\n",
    "data = pd.read_csv(\"./all_data.csv\")\n",
    "#filter out everything in balanced_data\n",
    "filtered_data = data[~data['id'].isin(balanced_data['id'])]\n",
    "\n",
    "#filter out id with less than 20 images\n",
    "filtered_data = filtered_data.groupby('id').filter(lambda x: len(x) >= 20)\n",
    "\n",
    "#check number of id per race\n",
    "id_per_race = filtered_data.groupby('race')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(filtered_data, \"./\", \"disjoint_balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample biased datasets for each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_by_race(race_data, id_per_race, image_per_id):\n",
    "    total_img = id_per_race * image_per_id\n",
    "\n",
    "    # Group by ID and count the number of images per ID\n",
    "    data_grouped = race_data.groupby(\"id\").count()\n",
    "    \n",
    "    # Get a list of IDs that meet the image count criteria\n",
    "    eligible_ids = data_grouped.index.tolist()\n",
    "    \n",
    "    # Shuffle the list of eligible IDs\n",
    "    random.shuffle(eligible_ids)\n",
    "    \n",
    "    # Take the first 'id_per_race' IDs to ensure uniform distribution\n",
    "    selected_ids = eligible_ids[:id_per_race]\n",
    "    \n",
    "    # Filter the DataFrame to include only rows with selected IDs\n",
    "    filtered_id_df = race_data[race_data[\"id\"].isin(selected_ids)]\n",
    "    \n",
    "    # Randomly sample images from each selected ID to meet the 'total_img' requirement\n",
    "    num_selected_images = 0\n",
    "    selected_rows = []\n",
    "    \n",
    "    for id in selected_ids:\n",
    "        id_df = filtered_id_df[filtered_id_df[\"id\"] == id]\n",
    "        num_images_for_id = min(image_per_id, total_img - num_selected_images)\n",
    "        \n",
    "        # Randomly sample 'num_images_for_id' images for the current ID\n",
    "        sampled_rows = id_df.sample(n=num_images_for_id, random_state=42)\n",
    "        selected_rows.extend(sampled_rows.values)\n",
    "        num_selected_images += num_images_for_id\n",
    "        \n",
    "        if num_selected_images >= total_img:\n",
    "            break\n",
    "    \n",
    "    # Create a new DataFrame from the selected rows\n",
    "    new_df = pd.DataFrame(selected_rows, columns=['image_name', 'id', 'race'])\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def generate_unbalanced_datasets(filtered_csv, output_dir, majority_num_ids=3600, minority_num_ids=780, image_per_id=20):\n",
    "    race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "    label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "    \n",
    "    filtered_data = pd.read_csv(filtered_csv)\n",
    "    coloumns = ['image_name', 'id', 'race']\n",
    "\n",
    "    for race, label in race_to_label.items():\n",
    "        cur_label = label\n",
    "        cur_biased_data = pd.DataFrame(columns=coloumns)\n",
    "\n",
    "        for iter_label, _ in label_to_race.items():\n",
    "            if iter_label == cur_label:\n",
    "                majority_race = filtered_data[filtered_data['race'] == cur_label]\n",
    "                majority_sample = sample_by_race(majority_race, majority_num_ids, image_per_id)\n",
    "                cur_biased_data = pd.concat([cur_biased_data, majority_sample])\n",
    "            else:\n",
    "                minority_sample = filtered_data[filtered_data['race'] == iter_label]\n",
    "                minority_sample = sample_by_race(minority_sample, minority_num_ids, image_per_id)\n",
    "                cur_biased_data = pd.concat([cur_biased_data, minority_sample])\n",
    "\n",
    "        # Save the unbalanced data as CSV.\n",
    "        cur_biased_data.to_csv(f'{output_dir}/unbalanced_{race}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_unbalanced_datasets(\"./disjoint_balanced_data.csv\", \"./\", majority_num_ids=3600, minority_num_ids=780, image_per_id=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "african_unbalnced = pd.read_csv(\"./unbalanced_African.csv\")\n",
    "asian_unbalnced = pd.read_csv(\"./unbalanced_Asian.csv\")\n",
    "caucasian_unbalnced = pd.read_csv(\"./unbalanced_Caucasian.csv\")\n",
    "indian_unbalnced = pd.read_csv(\"./unbalanced_Indian.csv\")\n",
    "\n",
    "balanced_data = pd.read_csv(\"./balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(african_unbalnced.groupby('id').count()['image_name'].value_counts())\n",
    "print(asian_unbalnced.groupby('id').count()['image_name'].value_counts())\n",
    "print(\"Caucasian unbalanced\", caucasian_unbalnced.groupby('id').count()['image_name'].value_counts())\n",
    "print(\"Indian unbalanced\", indian_unbalnced.groupby('id').count()['image_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_unbalnced.groupby('race')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check identity leakage\n",
    "african_unbalnced_id = african_unbalnced['id'].tolist()\n",
    "asian_unbalnced_id = asian_unbalnced['id'].tolist()\n",
    "caucasian_unbalnced_id = caucasian_unbalnced['id'].tolist()\n",
    "indian_unbalnced_id = indian_unbalnced['id'].tolist()\n",
    "\n",
    "balanced_data_id = balanced_data['id'].tolist()\n",
    "\n",
    "print(\"number of identity overlap between african_unbalnced and balanced_data: \", len(set(african_unbalnced_id).intersection(balanced_data_id)))\n",
    "print(\"number of identity overlap between asian_unbalnced and balanced_data: \", len(set(asian_unbalnced_id).intersection(balanced_data_id)))\n",
    "print(\"number of identity overlap between caucasian_unbalnced and balanced_data: \", len(set(caucasian_unbalnced_id).intersection(balanced_data_id)))\n",
    "print(\"number of identity overlap between indian_unbalnced and balanced_data: \", len(set(indian_unbalnced_id).intersection(balanced_data_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Train and Test in Balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_train_test(data, percentage_test_per_id=0.2):\n",
    "    image_per_id = balanced_data.groupby('id').count()['image_name'][0]\n",
    "    num_test_per_id = (image_per_id * percentage_test_per_id).astype(int)\n",
    "\n",
    "    result_df = data.copy()\n",
    "    result_df['split'] = 'train'\n",
    "    unique_ids = data['id'].unique()\n",
    "\n",
    "    for id in unique_ids:\n",
    "        test_indices = data[data['id'] == id].head(num_test_per_id).index\n",
    "        result_df.loc[test_indices, 'split'] = 'test'\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/lw4c28nj0t9673hql45sbnp80000gn/T/ipykernel_98387/218188292.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  image_per_id = balanced_data.groupby('id').count()['image_name'][0]\n"
     ]
    }
   ],
   "source": [
    "#ignor the warning, it works fine \n",
    "\n",
    "balanced_data = pd.read_csv(\"./balanced_data.csv\")\n",
    "african_balanced = balanced_data[balanced_data['race'] == 3]\n",
    "african_balanced_split = mark_train_test(african_balanced, 0.2)\n",
    "\n",
    "save_csv(african_balanced_split, \"./\", \"african_balanced_split.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train and test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_image(identity_selection, data_path, save_path, label_to_race):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # Make a copy of the identity_selection\n",
    "    identity_selection = identity_selection.copy()\n",
    "    if 'split' in identity_selection.columns:\n",
    "        identity_selection = identity_selection[identity_selection['split'] == 'train']\n",
    "\n",
    "    for _, row in tqdm(identity_selection.iterrows(), total=len(identity_selection), desc=\"Processing Images\"):\n",
    "        image_name = row['image_name']\n",
    "        id = row['id']\n",
    "        race = label_to_race[row['race']]\n",
    "\n",
    "        source_path = os.path.join(data_path, race, id, image_name)\n",
    "        identity_save_path = os.path.join(save_path, id)\n",
    "\n",
    "        # Create a subdirectory for the identity if it doesn't exist\n",
    "        if not os.path.exists(identity_save_path):\n",
    "            os.makedirs(identity_save_path)\n",
    "\n",
    "        destination_path = os.path.join(identity_save_path, image_name)\n",
    "        \n",
    "        # Check if the image already exists\n",
    "        if os.path.exists(destination_path):\n",
    "            continue\n",
    "\n",
    "        # Copy the image without converting\n",
    "        shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print('Finished copying training images to folders in JPG format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_image(identity_selection, data_path, save_path, label_to_race):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)    \n",
    "\n",
    "    identity_selection = identity_selection.copy()\n",
    "    \n",
    "    if 'split' in identity_selection.columns:\n",
    "        identity_selection = identity_selection[identity_selection['split'] == 'test']\n",
    "\n",
    "    for _, row in tqdm(identity_selection.iterrows(), total=len(identity_selection), desc=\"Processing Images\"):\n",
    "        image_name = row['image_name']\n",
    "        id = row['id']\n",
    "        race = label_to_race[row['race']]\n",
    "\n",
    "        source_path = os.path.join(data_path, race, id, image_name)\n",
    "\n",
    "        # Save to each race folder\n",
    "        race_path = os.path.join(save_path, race)\n",
    "        if not os.path.exists(race_path):\n",
    "            os.makedirs(race_path)\n",
    "\n",
    "        identity_save_path = os.path.join(race_path, id)\n",
    "\n",
    "        # Create a subdirectory for the identity if it doesn't exist\n",
    "        if not os.path.exists(identity_save_path):\n",
    "            os.makedirs(identity_save_path)\n",
    "\n",
    "        destination_path = os.path.join(identity_save_path, image_name)\n",
    "        \n",
    "        # Check if the image already exists\n",
    "        if os.path.exists(destination_path):\n",
    "            continue\n",
    "\n",
    "        # Copy the image without converting\n",
    "        shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print('Finished copying images to folders in JPG format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 48000/48000 [00:03<00:00, 14421.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished copying training images to folders in JPG format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 12000/12000 [00:00<00:00, 14139.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished copying images to folders in JPG format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#just saving one race for now\n",
    "data = pd.read_csv(\"./balanced_data_split.csv\")\n",
    "african_balanced_split = data[data['race']==3]\n",
    "caucasian_balanced_split = data[data['race']==0]\n",
    "\n",
    "african_and_cau = pd.concat([african_balanced_split, caucasian_balanced_split])\n",
    "\n",
    "\n",
    "data_folder_path = \"./race_per_7000\"\n",
    "save_path = \"./\"\n",
    "\n",
    "\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "save_train_image(african_and_cau, data_folder_path, save_path+'/train', label_to_race)\n",
    "save_test_image(african_and_cau, data_folder_path, save_path+'/test', label_to_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#just getting mvp mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_unbalanced = pd.read_csv(\"./unbalanced_African.csv\")\n",
    "\n",
    "#take first 50 id from african and first 10 caucasian \n",
    "first_50_african_uniq_id = african_unbalanced[african_unbalanced['race']==3]['id'].unique()[:50]\n",
    "african_filter = african_unbalanced[african_unbalanced['id'].isin(first_50_african_uniq_id)]\n",
    "\n",
    "first_10_cau_uniq_id = african_unbalanced[african_unbalanced['race']==0]['id'].unique()[:10]\n",
    "cau_filter = african_unbalanced[african_unbalanced['id'].isin(first_10_cau_uniq_id)]\n",
    "\n",
    "#combine\n",
    "data = pd.concat([african_filter, cau_filter])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'split'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m race_to_label \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mCaucasian\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIndian\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAsian\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAfrican\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m label_to_race \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mCaucasian\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mIndian\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mAsian\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m3\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mAfrican\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m save_train_image(data, data_folder_path, save_path\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/con_train_mini\u001b[39;49m\u001b[39m'\u001b[39;49m, label_to_race)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# save_test_image(data, data_folder_path, save_path+'/test', label_to_race)\u001b[39;00m\n",
      "\u001b[1;32m/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb Cell 33\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Make a copy of the identity_selection\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m identity_selection \u001b[39m=\u001b[39m identity_selection\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m identity_selection \u001b[39m=\u001b[39m identity_selection[identity_selection[\u001b[39m'\u001b[39;49m\u001b[39msplit\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m tqdm(identity_selection\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(identity_selection), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing Images\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     image_name \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mimage_name\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'split'"
     ]
    }
   ],
   "source": [
    "\n",
    "#get dataset\n",
    "#just saving one race for now\n",
    "\n",
    "data_folder_path = \"./race_per_7000\"\n",
    "save_path = \"./\"\n",
    "\n",
    "\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "save_train_image(data, data_folder_path, save_path+'/con_train_mini', label_to_race)\n",
    "# save_test_image(data, data_folder_path, save_path+'/test', label_to_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
